# 手动配置调优 {#concept_qc2_khm_bgb .concept}

本文为您介绍实时计算作业手动配置调优。

**说明：** 建议您完成作业反压检测后，再判断是否需要进行配套调优。实时计算3.0以上版本作业反压检测方法，请参见[作业反压检测步骤](https://help.aliyun.com/knowledge_detail/124589.html)。

## 手动配置调优概述 {#section_q1g_lhm_bgb .section}

手动配置调优的内容主要有3种类型：

-   作业参数调优：主要对作业中的miniBatch等参数进行调优。
-   资源调优：主要对作业中的Operator的并发数（Parallelism）、CPU（Core）、堆内存（Heap Memory）等参数进行调优。
-   上下游参数调优：主要对作业中的上下游存储参数进行调优。

下面通过3个章节对以上3类调优进行介绍，参数调优后将生成新的配置，Job需重新上线启动/恢复才能使用新的配置，启动新配置的方法请参见[重新启用新的配置](#section_isd_5nm_bgb)。

## 作业参数调优 {#section_sxj_yjm_bgb .section}

miniBatch设置：该设置只能优化GROUP BY。Flink SQL流模式下，每来一条数据都会执行State操作，IO消耗较大，设置miniBatch后，同一个Key的一批数据只访问一次State，且只输出最新的一条数据，即减少了State访问也减少了向下游的数据更新。miniBatch设置如下：

**说明：** 

-   新增加的作业参数建议用户停止重启。
-   更新作业参数值暂停恢复即可。

``` {#codeblock_3ef_q00_mz6}
# excatly-once语义。
blink.checkpoint.mode=EXACTLY_ONCE
# checkpoint间隔时间，单位毫秒。
blink.checkpoint.interval.ms=180000
blink.checkpoint.timeout.ms=600000
# 实时计算2.0及以上版本使用niagara作为statebackend，以及设定state数据生命周期，单位毫秒。
state.backend.type=niagara
state.backend.niagara.ttl.ms=129600000
# 实时计算2.0及以上版本开启5秒的microbatch（窗口函数不要设置这个参数。）
blink.microBatch.allowLatencyMs=5000
# 表示整个Job允许的延迟。
blink.miniBatch.allowLatencyMs=5000
# 单个Batch的size。
blink.miniBatch.size=20000
# local优化，实时计算2.0及以上版本默认已经开启，1.6.4需手动开启。
blink.localAgg.enabled=true
# 实时计算2.0及以上版本开启partial优化，解决count distinct效率低问题。
blink.partialAgg.enabled=true
# union all优化。
blink.forbid.unionall.as.breakpoint.in.subsection.optimization=true
# GC优化（源表为SLS时，不能设置改参数）。
blink.job.option=-yD heartbeat.timeout=180000 -yD env.java.opts='-verbose:gc -XX:NewRatio=3 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:ParallelGCThreads=4'
# 时区设置。
blink.job.timeZone=Asia/Shanghai
```

[![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289714933888_zh-CN.png)

](http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/62491/cn_zh/1513165425069/119.png)

## 资源调优 {#section_dj3_clm_bgb .section}

下文通过示例为您介绍资源调优方法。

1.  分析问题
    1.  通过Job的拓扑图查看到2号的Task节点的输入队列已达到100%，造成上游1号节点的数据堆积，输出队列造成数据堆积。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289714933889_zh-CN.png)

    2.  单击2号的Task节点，找到队列已达到100%的TaskExecutor。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289714933912_zh-CN.png)

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289714933891_zh-CN.png)

    3.  查看TaskExecutor的CPU和内存的使用量，根据使用量来增加相应的CPU和MEM。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715033892_zh-CN.png)

2.  性能调优

    1.  进入调优窗口。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715033893_zh-CN.png)

    2.  打开可视化编辑。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715033894_zh-CN.png)

    3.  找到2号Task对应的Group或Operator，可以按Group批量修改或对单个Operator进行参数修改。
        -   按Group批量修改。

            ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715033895_zh-CN.png)

            ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715133896_zh-CN.png)

        -   单个Operator修改。

            ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715133897_zh-CN.png)

            ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715133898_zh-CN.png)

    4.  找到对应的Operator进行修改。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715133899_zh-CN.png)

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715233900_zh-CN.png)

    5.  配置参数后单击右上角的**应用当前配置并关闭窗口**，保存当前配置。
    **说明：** 

    如果增加了Group的资源配置后，作业的运行效率提升不明显，需要按照以下顺序分析问题：

    1.  该节点是否存在数据倾斜现象。
    2.  复杂运算的Operator节点（如：GROUP BY、WINDOW、JOIN等）的子节点是否存在异常。

        Operator子节点拆解方法：

        1.  单击需要修改的Operator。
        2.  将chainingStrategy参数值修改为HEAD。若已为HEAD，需要将后一个Operator的chainingStrategy参数值修改为HEAD。
         chainingStrategy可选参数值如下：

        -   ALWAYS：代表把单个的节点合并成一个大的GROUP里面。
        -   NEVER：保持不变。
        -   HEAD ：表示把在合并成一个GROUP里面单个节点查分出来。
3.  资源参数的配置原则和建议

    作业的资源配置建议为：`core:mem=1:4`，即1个核对应4G内存。

    **说明：** 

    -   Operator的总core=并发数\*core。
    -   Operator的总mem=并发数\*heap\_mem。
    -   Group中的core取各个Operator中最大值，mem取各个Operator中mem之和。
    例如，core参数值为1CU，mem参数值为3G，则最终资源分配结果的是1CU+4G；core参数值为1CU，mem参数值为5G，则最终资源分配结果的是1.25CU+5G。

    -   可调参数
        -   parallelism
            -   source节点

                **说明：** source的并发不能大于source的shard数。

                -   source数量和上游Partition数量相关。
                -   例如，source的个数是16，source的并发数可以配置为16、8或4等，不得超过16。
            -   中间节点
                -   根据预估的QPS计算。
                -   QPS低的任务，中间处理节点数和source并发数一样。
                -   QPS高的任务，中间处理节点数配置为比source并发数大，例如，64、128或者256。
            -   sink节点
                -   并发度和下游存储的Partition数相关，一般是下游Partition个数的2~3倍。
                -   如果配置过大会导致输入超时或失败。例如，下游sink节点个数是16，建议sink的并发数最大配置为48。
        -   core

            默认值为0.1，根据实际CPU使用配置（但最好能被1整除），建议参数值为0.25。

        -   heap\_memory

            堆内存，默认为256MB，根据实际内存使用配置。

    -   存在GROUP BY的Task节点中可配置参数state\_size：state的大小，默认值为0。存在GROUP BY的Task节点需要把state\_size参数值设置为`1`，表示该Operator会使用state功能，作业会为该Operator申请额外的内存。如果state\_size参数值不设置为`1`，作业可能运行失败。state\_size需要配成1的Operator有：GROUP BY 、undefinedJOIN、OVER和WINDOW。

## 上下游参数调优 {#section_wml_snm_bgb .section}

由于实时计算的特性，每条数据均会触发上下游存储的读写，会对上下游存储形成性能压力。通过设置batchsize批量的读写上下游存储数据可以降低对上下游存储的压力。支持batchsize参数的上下游存储如下：

|名称|参数|详情|设置参数值|
|DataHub源表|batchReadSize|单次读取条数|可选，默认为10。|
|DataHub源表|batchSize|单次写入条数|可选，默认为300。|
|日志服务（Log Service）源表|batchGetSize|单次读取logGroup条数|可选，默认为10。|
|分析型数据库（AnalyticDB）结果表|batchSize|每次写的批次大小|可选，默认为1000。|
|云数据库（RDS）结果表|batchSize|每次写的批次大小|可选，默认为50。|
|云数据库HybridDB for MySQL（petaData）结果表|batchSize|每次写的批次大小|可选，默认值1000 ，表示每次写多少条，建议最大设置4096。|
|bufferSize|去重的buffer大小，需要指定主键才生效。|可选，设置batchSize必须设置bufferSize，建议最大设置4096。|

**说明：** DDL的WITH参数列中增加batchSize相关参数，即可完成数据的批量读写设置。例如，```batchReadSize='<number>'`。

## 重新启用新的配置 {#section_isd_5nm_bgb .section}

完成配置后，需要重新启动/恢复作业才能使新配置生效。

1.  上线作业。**配置方式**选择**使用上次资源配置** 。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715235713_zh-CN.png)

2.  暂停原作业。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715235714_zh-CN.png)

3.  恢复原作业。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715235715_zh-CN.png)

4.  选择**按最新配置恢复**。

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41064/156289715235718_zh-CN.png)

5.  重新恢复后，可通过**运维** \> **运行信息** \> **Vertex拓扑**查看新的配置是否生效。

**说明：** 

一般不建议采用先停止再启动的方式触发新配置。Job停止后status状态会消除，可能会导致计算结果不一致。

## 相关名词解释 {#section_s12_c4m_bgb .section}

-   Global
    -   isChainingEnabled ：是否启用Chain策略，默认为true，**不需要修改**。
-   Nodes
    -   id：节点ID号，自动生成，ID号唯一，**不需要修改**。
    -   uid： 节点UID号，用于计算Operator ID，如果不设置，会使用ID。
    -   pact：节点类型，例如，Data Source、Operator或Data Sink等，**不需要修改**。
    -   name：节点名字，可自定义。
    -   slotSharingGroup：`default`，**不需要修改**。
    -   chainingStrategy：chain的策略，有 HEAD、ALWAYS和NEVER，**根据需要修改**。
    -   parallelism：并发度，默认为`1`，可以根据实际数据量调大。
    -   core：CPU，默认为`0.1`，根据实际CPU使用配置，建议设置为`0.25`。
    -   heap\_memory：堆内存，默认为256MB，根据实际内存使情况进行用配置。
    -   direct\_memory：JVM堆外内存，默认`0`，建议不修改。
    -   native\_memory：JVM堆外内存，JNI使用，默认为`0`，建议用10MB。
-   Chain

    Flink SQL任务是一个DAG图，由多个节点（Operator）组成，部分上下游的节点在运行时可以合成为一个节点，称为Chain。Chain后的节点，总CPU为所有节点CPU的最大值，总内存为所有节点内存的总和。多节点合成一个节点可以有效的减少网络传输，降低成本。

    **说明：** 

    -   并发数相同的节点才能Chain。
    -   Group By节点不能Chain。

