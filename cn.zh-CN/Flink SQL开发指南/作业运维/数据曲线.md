# 数据曲线 {#concept_62483_zh .concept}

阿里云实时计算提供了当前作业的核心指标概览页面。您可以通过数据曲线对作业的运行情况的进行一键式的诊断。未来实时计算还会提供更多基于作业现状的深度智能分析算法，以辅助您进行智能化和自动化诊断。

数据曲线示例如下。

![Curve Charts](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41067/156341311433954_zh-CN.png)

**说明：** 

-   数据曲线指标在实时计算作业运行状态下才提供显示，暂停以及停止状态均不提供显示。
-   作业指标是实时计算系统异步后台采集，存在一定延迟。作业启动1分钟后，各项指标才能逐步采集，然后在数据曲线显示。

## OverView {#section_k5k_pjr_bgb .section}

-   **Failover** 

    Failover曲线显示当前Job出现Failover（错误或者异常）的频率。计算方法为当前Failover时间点的前1分钟内出现Failover的累计次数除以60。（例如 ，最近1分钟Failover了一次，Failover的值为1/60=0.01667。）

-   **Delay** 

    为了全面的了解实时计算全链路的时效状况和作业的性能，实时计算提供3种延时指标：

    -   **业务延时**（Processing Delay）：业务延迟 = 当前系统时间 – 当前系统处理的最后一条数据的事件时间（Event time）。如果后续没有数据再进入上游存储，由于当前系统时间在不断往前推进，业务延时也会随之逐渐增大。
    -   **数据滞留时间**（Data Pending Time）：数据滞留时间 = 数据进入实时计算的时间 - 数据事件时间（Event time）。即使后续没有数据再进入上游存储，数据滞留时间也不会随之逐渐增大。通常用数据滞留时间来评估当前实时计算作业是否存在反压。
    -   **数据间隔时间**（Data Arrival Interval）：数据间隔时间= 业务延迟– 数据滞留时间。实时计算没有反压（即数据滞留时间较小且平稳）时，数据间隔时间可以反映数据源数据间的稀疏程度；实时计算存在反压（即数据滞留时间较大或不平稳）时，此参数没有实质性参考意义。
    **说明：** 

    -   实时计算是分布式计算框架，以上3类延时指标的Metric首先是针对Source的单个分区（Shard/Partition等）进行计算，然后汇报所有分区中的最大值呈现到前端页面上，因此前端页面上显示的汇聚后的数据间隔时间并不精确等于业务延时 – 数据滞留时间。
    -   如果Source中的某个分区没有新的数据，将会导致业务延迟逐渐增大。
    -   目前底层算法实现时，当数据间隔时间小于10秒时，会将数据间隔时间设置为0，进行上报。
-   **各Source的TPS数据输入** 

    **各Source的TPS数据输入**对实时计算作业所有的流式数据输入进行统计，记录每秒读取数据源表的Block的数，让您直观的了解数据存储TPS（Transactions Per Second）的情况。与RPS（Record Per Second）不同，RPS是读取数据源TPS的Block数解析后的数据，单位是条/秒。（例如，日志服务，1秒读取5个LogGroup，那么TPS=5，如果每个LogGroup解析出来8个日志记录，那么一共解析出40个日志记录，RPS=40。）

-   **各Sink的数据输出** 

    **各Sink的数据输出**对实时计算作业所有的数据输出（并非是流式数据存储，而是全部数据存储）做出进行统计，让您直观的了解数据存储RPS（Record Per Second）的情况。通常，在系统运维过程中，如果出现没有数据输出的情况，除了检查上游是否存在数据输入，同样要检查下游是否真的存在数据输出。

-   **各Source的RPS数据输入** 

    **各Source的RPS数据输入**对实时计算作业所有的流式数据输入进行统计，让您直观的了解数据存储RPS（Record Per Second）情况。通常，在系统运维过程中，如果出现没有数据输出的情况，需要查看该值，判断是否数据源输出数据存在异常。

-   **各Source的数据流量输入****各Source的数据流量输入**对实时计算作业所有的流式数据输入进行统计，记录每秒读取输入源表的流量的统计，让您直观的了解数据流量BPS（Byte Per Second）情况。
-   **各Source的脏数据****各Source的脏数据**为您显示实时计算Source各时间段脏数据条数。
-   **Auto Scaling Successes and Failures** 

    **说明：** 仅实时计算3.0.0以上版本支持此曲线。

     **Auto Scaling Successes and Failures**为您显示AutoScale成功执行和未成功执行的次数。

-   **CPUs Consumed By Auto Scaling** 

    **说明：** 仅实时计算3.0.0以上版本支持此曲线。

     **CPUs Consumed By Auto Scaling**为您显示执行AutoScale时消耗的CPU量。

-   **Memory Consumed By Auto Scaling** 

    **说明：** 仅实时计算3.0.0以上版本支持此曲线。

     **Memory Consumed By Auto Scaling**为您显示执行AutoScale时消耗的内存量。


## Advanced View {#section_a7a_dy9_1ao .section}

阿里云实时计算提供可以恢复数据流应用到一致状态的容错机制。容错机制的核心就是持续创建分布式数据流及其状态的一致快照。这些快照在系统遇到故障时，充当可以回退的一致性检查点（checkpoint）。

分布式快照的核心概念之一就是数据栅栏（barrier）。这些barrier被插入到数据流中，作为数据流的一部分和数据一起向下流动。barrier不会干扰正常数据，数据流严格有序。一个barrier把数据流分割成两部分：一部分进入到当前快照，另一部分进入下一个快照。每一个barrier都带有快照 ID，并且barrier之前的数据都进入了此快照。barrier不会干扰数据流处理，所以非常轻量。多个不同快照的多个barrier会在流中同时出现，即多个快照可能同时创建。

![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41067/156341311433962_zh-CN.png)

barrier在数据源端插入，当snapshot n的barrier插入后，系统会记录当前snapshot位置值 n （用Sn表示）。然后barrier继续往下流动，当一个operator从其输入流接收到所有标识snapshot n的barrier时，它会向其所有输出流插入一个标识snapshot n的barrier。当sink operator （DAG 流的终点）从其输入流接收到所有barrier n时，operator向检查点协调器确认snapshot n已完成。当所有sink都确认了这个快照，快照就被标识为完成。

![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41067/156341311433963_zh-CN.png)

以下就是记录Checkpoint的各种参数配置。

|数据曲线名称|说明|
|------|--|
|Checkpoint Duration|进行Checkpoint保存状态所花费的时间，单位为毫秒。|
|CheckpointSize|进行Checkpoint所消耗的内存大小。|
|CheckpointAlignmentTime|当前节点进行checkpoint操作等待上游所有节点到达当前节点的时间。当sink operator（DAG 流的终点）从其输入流接收到所有barrier n时，它向the checkpoint coordinator确认snapshot n已完成。当所有sink都确认了这个快照，快照就被标识为完成。这个等待的时间就是checkpointAlignmentTime。|
|CheckpointCount|指定时间段内Checkpoint的数量|
|Get|指定时间段内每个SubTask对ROCKSDB进行Get操作所花费的时间（最大值）。|
|Put|指定时间段内每个SubTask对ROCKSDB进行Put操作所花费的时间（最大值）。|
|Seek|指定时间段内每个SubTask对ROCKSDB进行Seek操作所花费的时间（最大值）。|
|State Size|指定时间段内Job内部State存储大小（如果增量过快JOB是异常的）。|
|CMS GC Time|指定时间段内Job内部容器进行GC（Garbage Collection）花费的时间。|
|CMS GC Rate|指定时间段内Job内部容器进行GC（Garbage Collection）的频率。|

## WaterMark {#section_j5r_9kj_mge .section}

|数据曲线名称|说明|
|------|--|
|WaterMark Delay|WaterMark距离系统时间的差值。|
|数据迟到丢弃TPS|每秒丢弃晚于Watermark时间到达Window的数据的数量。|
|数据迟到累计丢弃数|累计丢弃晚于Watermark时间到达Window的数据的数量。|

## Delay {#section_3w8_kot_877 .section}

 **Source SubTask 最大延迟 Top 15**

![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/41067/156341311433978_zh-CN.png)

表示每个Source的并发的业务延时的时长。

## Throughput {#section_tsl_p41_94s .section}

|数据曲线名称|说明|
|------|--|
|Task Input TPS|作业中所有的Task的数据的输入状况。|
|Task Output TPS|作业中所有的Task的数据的输出状况。|

## Queue {#section_1q4_1jo_2z4 .section}

|数据曲线名称|说明|
|------|--|
|Input Queue Usage|作业中所有的Task的数据的输入队列。|
|Output Queue Usage|作业中所有的Task的数据的输出队列。|

## Tracing {#section_yr6_st8_58y .section}

|数据曲线名称|说明|
|------|--|
|Time Used In Processing Per Second|处理Task中每秒数据所花费的时长。|
|Time Used In Waiting Output Per Second|等待Task中每秒数据输出所花费的时长。|
|TaskLatency Histogram Mean|每个Task的延时时长。|
|Wait Output Histogram Mean|每个Task的等待输出时长。|
|Wait Input Histogram Mean|每个Task的等待输入时长。|
|Partition Latency Mean|每个分区中并发的延时时长。|

## Process {#section_l8m_4xm_m5q .section}

|数据曲线名称|说明|
|------|--|
|Process Memory RSS|每个进程内存的使用状况。|
|CPU Usage|每个进程CPU的使用状况。|

## JVM {#section_ibi_fsi_p37 .section}

|数据曲线名称|说明|
|------|--|
|Memory Heap Used|Job使用的JVM Heap存储量。|
|Memory Non-Heap Used|Job使用的JVM 非Heap存储量。|
|Thread Count|Job的线程数。|
|GC\(CMS\)|Job完成CG（Garbage Collection）的次数。|

